# Optimization




**LOSS FUNCTION**

We need to know how far our model is from the actual values. This function measures how far an estimated value of a quantity is from the true value. 

**RISK FUNCTION**

We calculate the risk function by taking the expected value of the loss function.

**Arg Min**

We are looking for a function h from a pool of functions H that minimizes the risk function.

## Gradient descent
## Stochastic (and mini-batch) gradient descent
------------
## Regularization

To reduce variance error, it is often necessary to simplify the model, for example by reducing the number of features or parameters, or by regularizing the model. Another approach is to increase the size of the training dataset, which can help the model learn the underlying patterns more accurately and reduce its sensitivity to noise.

## What is regularization?
Constrain the parameter space by adding an additional loss term on the model parameters. With this weight penalty, parameters can no longer vary freely


### 1. L1 (Lasso) regularization
### 2. L2 (Ridge) regularization
### 3.Elastic Net regularization


