## Generalization Error

There are the three main things that can go wrong during the training of ML model which contribute to the overall generalization error:

- Approximation Error
- Estimation Error
- Optimization Error


### Funamental Tradeoff:
- **Overfit**: Model is too complex
    - Training score high and test score low
    - It can result from **Variance Error** (to be sensitive to small fluctuations)
- **Underfit**: Model is very simple
    - Training score low and test score low
    - It can result from **Bias Error** (high bias can cause an algorithm to miss important relations between features and target variable)

![img](https://miro.medium.com/max/2250/1*_7OPgojau8hkiPUiHoGK_w.png)

### Approximation Error

If we choose an H (the pool of functions h we can choose from) or an l which are too simple, the accuracy of our model will be negatively affected by the approximation error.

For example, if we have an image classification task and we choose the H as any logistic regression (or another model not complex enough for this kind of task). In this case, no matter how much data we have, we will end up with a poor model because of a big approximation error.

### Estimation Error

We deal with the estimation error when we don't feed the model with enough data. Complex functions require more data to learn properly. Let's take a look at the example from before:

We have an image classification task and we correctly choose to train a model with Deep Neural Networks but we only have 100 pictures in our training set. No matter what we do, our model will be weak because of a huge estimation error.

### Optimization Error

The optimization error occurs when we have a loss function which is too complex, and as a result we don't find the optimal solution. A huge amount of observations can increase the optimization error as well.

Sometimes, in real life, we can even train a model on a sample only to make sure that we have the right amount of data.