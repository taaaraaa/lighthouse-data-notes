## Generalization Error

There are the three main things that can go wrong during the training of ML model which contribute to the overall generalization error:

- Approximation Error
- Estimation Error
- Optimization Error


Data has two components components: signal (pattern) + noise
- Example: predicting house prices from # of bedrooms, area, age, etc.

**Signal** : degree to which these features actually influence the price.

**Noise**: spurious relationships between these features and the price in your dataset.
> :memo: **Note:**  Goal of machine learning: learn the signal, ignore the noise

## Funamental Tradeoff:
- **Overfit**: Model is too complex! The model is learning the noise!
    - Training score high and test score low
    - It can result from **Variance Error** (to be sensitive to small fluctuations)
- **Underfit**: Model is very simple
    - Training score low and test score low
    - It can result from **Bias Error** (high bias can cause an algorithm to miss important relations between features and target variable)

1. ***Var Error***: How the model performance is varing in the test data from the train data! It is possible to estimate the model's variance error by comparing the performance of the model on the training set and the validation set. In this case the model performs significantly better on the training set than on the validation set.
2. Bias Error: How well the model performes on the train data minus the human error (also know as unavoidable error).

> :memo: **Note:** Bias**2 error is related to **train error**!

> :memo: **Note:** Variance error is related to **test error**!

![img](https://miro.medium.com/max/2250/1*_7OPgojau8hkiPUiHoGK_w.png)

!['Bias-Variance Tradeoff'](../data/bias-var-tradeoff.png 'Bias-Variance Tradeoff')
### Approximation Error

If we choose an H (the pool of functions h we can choose from) or an l which are too simple, the accuracy of our model will be negatively affected by the approximation error.

For example, if we have an image classification task and we choose the H as any logistic regression (or another model not complex enough for this kind of task). In this case, no matter how much data we have, we will end up with a poor model because of a big approximation error.

### Estimation Error

We deal with the estimation error when we don't feed the model with enough data. Complex functions require more data to learn properly. Let's take a look at the example from before:

We have an image classification task and we correctly choose to train a model with Deep Neural Networks but we only have 100 pictures in our training set. No matter what we do, our model will be weak because of a huge estimation error.

### Optimization Error

The optimization error occurs when we have a loss function which is too complex, and as a result we don't find the optimal solution. A huge amount of observations can increase the optimization error as well.

Sometimes, in real life, we can even train a model on a sample only to make sure that we have the right amount of data.

:memo: **Note:** Training and test datasets are essential components of the model development process. They are used to evaluate the performance of a model and to tune its hyperparameters

----------

